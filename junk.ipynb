{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding to all object columns\n",
    "label_encoders = {}\n",
    "for column in heart_df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    heart_df[column] = le.fit_transform(heart_df[column].astype(str))\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Display the first few rows of the transformed dataset\n",
    "heart_df.head(10)\n",
    "\n",
    "# Check the unique values in the 'num' column\n",
    "print(heart_df['num'].unique())\n",
    "\n",
    "# Map 'num' to binary (e.g., 0: no disease, 1: disease present)\n",
    "heart_df['num'] = heart_df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Verify the changes\n",
    "print(heart_df['num'].unique())\n",
    "\n",
    "\n",
    "logistic_model = logit(\"num ~ age + sex + dataset + cp + trestbps + chol + fbs + restecg + thalch + exang + oldpeak + slope + ca + thal\", \n",
    "                       data=heart_df).fit()\n",
    "print(logistic_model.summary())\n",
    "\n",
    "logistic_model = logit(\"num ~ sex + cp + chol + exang + oldpeak + slope + ca\", data=heart_df).fit()\n",
    "print(logistic_model.summary())\n",
    "\n",
    "\n",
    "X_logistic = heart_df[['sex', 'cp', 'chol', 'exang', 'oldpeak', 'slope', 'ca']]\n",
    "y_logistic = heart_df[\"num\"]\n",
    "\n",
    "\n",
    "predictions = logistic_model.predict(X_logistic) > 0.5 \n",
    "cf_matrix = confusion_matrix(y_logistic, predictions)\n",
    "accuracy = accuracy_score(y_logistic, predictions)\n",
    "\n",
    "print(cf_matrix)\n",
    "print(\"Accuracy of the new Model: \",accuracy)\n",
    "\n",
    "\n",
    "#split the data into X and Y\n",
    "X = heart_df.drop(['num','id','age_bins'], axis=1)\n",
    "y = heart_df['num']\n",
    "\n",
    "print (X)\n",
    "print (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'XG Boost': XGBClassifier(random_state=42)\n",
    "    \n",
    "}\n",
    "\n",
    "params = {\n",
    "    'Random Forest': {\n",
    "        'model__n_estimators': [100, 200, 300],\n",
    "        'model__max_depth': [ 10,20],\n",
    "        'model__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.1, 0.01],\n",
    "        'model__max_depth': [3, 5]\n",
    "    },\n",
    "    \n",
    "    'Logistic Regression': {\n",
    "        'model__C': [1, 10],\n",
    "        'model__solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model__n_neighbors': [3, 5],\n",
    "        'model__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    \n",
    "    'XG Boost': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.1, 0.01],\n",
    "        'model__max_depth': [3, 5]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize best model tracking\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Create a pipeline with the model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Get hyperparameters for the current model\n",
    "    model_params = params.get(name, {})\n",
    "\n",
    "    # Create GridSearchCV with the pipeline and parameters\n",
    "    grid_search = GridSearchCV(pipeline, model_params, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "    # Fit the pipeline\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"{name} - Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{name} - Best Score: {grid_search.best_score_}\")\n",
    "    print(f\"{name} - Test Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"{name} - Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"{name} - Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "    print('\\n')\n",
    "    \n",
    "    if accuracy_score(y_test, y_pred) > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_test, y_pred)\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "# print the best model & accuracy\n",
    "print(f\"The Best model is {best_model.named_steps['model']} with an accuracy of {best_accuracy*100}%\")\n",
    "\n",
    "\n",
    "# Training Random Forest...\n",
    "# Random Forest - Best Parameters: {'model__max_depth': 20, 'model__min_samples_split': 5, 'model__n_estimators': 100}\n",
    "# Random Forest - Best Score: 0.8484671302149177\n",
    "# Random Forest - Test Accuracy: 0.8235294117647058\n",
    "# Random Forest - Confusion Matrix:\n",
    "# [[83 18]\n",
    "#  [15 71]]\n",
    "# Random Forest - Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.85      0.82      0.83       101\n",
    "#            1       0.80      0.83      0.81        86\n",
    "\n",
    "#     accuracy                           0.82       187\n",
    "#    macro avg       0.82      0.82      0.82       187\n",
    "# weighted avg       0.82      0.82      0.82       187\n",
    "\n",
    "\n",
    "\n",
    "# Training Gradient Boosting...\n",
    "# Gradient Boosting - Best Parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 200}\n",
    "# Gradient Boosting - Best Score: 0.8378634639696585\n",
    "# Gradient Boosting - Test Accuracy: 0.8235294117647058\n",
    "# Gradient Boosting - Confusion Matrix:\n",
    "# [[83 18]\n",
    "#  [15 71]]\n",
    "# Gradient Boosting - Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.85      0.82      0.83       101\n",
    "#            1       0.80      0.83      0.81        86\n",
    "\n",
    "#     accuracy                           0.82       187\n",
    "#    macro avg       0.82      0.82      0.82       187\n",
    "# weighted avg       0.82      0.82      0.82       187\n",
    "\n",
    "\n",
    "\n",
    "# Training Logistic Regression...\n",
    "# Logistic Regression - Best Parameters: {'model__C': 1, 'model__solver': 'lbfgs'}\n",
    "# Logistic Regression - Best Score: 0.8378476611883692\n",
    "# Logistic Regression - Test Accuracy: 0.8235294117647058\n",
    "# Logistic Regression - Confusion Matrix:\n",
    "# [[84 17]\n",
    "#  [16 70]]\n",
    "# Logistic Regression - Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.84      0.83      0.84       101\n",
    "#            1       0.80      0.81      0.81        86\n",
    "\n",
    "#     accuracy                           0.82       187\n",
    "#    macro avg       0.82      0.82      0.82       187\n",
    "# weighted avg       0.82      0.82      0.82       187\n",
    "\n",
    "\n",
    "\n",
    "# Training K-Nearest Neighbors...\n",
    "# K-Nearest Neighbors - Best Parameters: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "# K-Nearest Neighbors - Best Score: 0.8164348925410871\n",
    "# K-Nearest Neighbors - Test Accuracy: 0.8074866310160428\n",
    "# K-Nearest Neighbors - Confusion Matrix:\n",
    "# [[80 21]\n",
    "#  [15 71]]\n",
    "# K-Nearest Neighbors - Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.84      0.79      0.82       101\n",
    "#            1       0.77      0.83      0.80        86\n",
    "\n",
    "#     accuracy                           0.81       187\n",
    "#    macro avg       0.81      0.81      0.81       187\n",
    "# weighted avg       0.81      0.81      0.81       187\n",
    "\n",
    "\n",
    "\n",
    "# Training XG Boost...\n",
    "# XG Boost - Best Parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100}\n",
    "# XG Boost - Best Score: 0.8378950695322376\n",
    "# XG Boost - Test Accuracy: 0.8235294117647058\n",
    "# XG Boost - Confusion Matrix:\n",
    "# [[84 17]\n",
    "#  [16 70]]\n",
    "# XG Boost - Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.84      0.83      0.84       101\n",
    "#            1       0.80      0.81      0.81        86\n",
    "\n",
    "#     accuracy                           0.82       187\n",
    "#    macro avg       0.82      0.82      0.82       187\n",
    "# weighted avg       0.82      0.82      0.82       187\n",
    "\n",
    "\n",
    "\n",
    "# The Best model is RandomForestClassifier(max_depth=20, min_samples_split=5, random_state=42) with an accuracy of 82.35294117647058%\n",
    "# Training Stacking Classifier...\n",
    "# Stacking Classifier - Test Ac\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Defining the base models\n",
    "base_estimators = [\n",
    "    ('random_forest', RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators= 300,\n",
    "        max_depth=10,\n",
    "        min_samples_split=2\n",
    "    )),\n",
    "    ('logistic_regression', LogisticRegression(\n",
    "        random_state=42, \n",
    "        solver='lbfgs',\n",
    "        C=1\n",
    "    ))\n",
    "]\n",
    "\n",
    "\n",
    "# Defining the final estimator (XGBoost)\n",
    "final_estimator = XGBClassifier(\n",
    "    random_state=42, \n",
    "    n_estimators=200, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "# Creating the Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=final_estimator,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "print(\"Training Stacking Classifier...\")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacking classifier\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Stacking Classifier - Test Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
    "print(\"Stacking Classifier - Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_stack))\n",
    "print(\"Stacking Classifier - Classification Report:\\n\", classification_report(y_test, y_pred_stack))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
